---
title: Why single-cell analysis? 
layout: background
permalink: /sci/
author: Ryan Mulqueen
category: background
---


Bulk methods to capture chromatin accessibility<sup>1</sup>, chromatin conformation<sub>2</sub>, DNA methylation<sub>3</sub> and others have increased our understanding of cell state diversity and the interplay between these features<sub>4,5</sub>. In actively developing samples, or on samples of complex tissue, bulk assays fall short. By capturing and processing thousands to millions of cells together, signals are merged to an average, covering up cellular diversity and genomic decision points. To specify the region, cell type, or developmental stages linking neurotypical cortical development with disorders, efforts have been made to take sequential samples for a time-course analysis, or to perform micro-dissections of the cortex<sub>6,7</sub>. These two approaches, while able to garner critical information, still lack the granularity to catalog a causative string of events for cell fate decisions, leaving the development of new single-cell methods as a promising recourse.

![Figure 1]({{site.baseurl}}/assets/images/fig1.png){:width="20%"}

Single-cell applications, the assessment of genomic or epigenomic profiles from discrete single cells, exists to address the shortcomings of those previous experimental designs. Sampling one cell at a time has two major benefits. The first is a less biased count of heterogeneity in a sample. For instance, previous post-mortem analysis demonstrated a large variance of cell type proportions across individual cortical samples. This heterogeneity has been known to skew analysis in bulk samples<sub>8</sub>. A second approach would be to isolate cells through a marker. However this approach can introduce bias into systems, especially when studying cell state transitions<sub>9</sub>. Through capturing cells in an unbiased manner and subsetting data to cells of interest post hoc, this effect can be mitigated and assumptions before data acquisition are limited — isolation of a single cell type allows for a more powerful case-control comparison. For instance, single-cell RNA libraries generated from 48 individuals with Alzheimer’s disease pathology uncovered many more differences using pairwise comparisons respective of cell types than have been previously uncovered in bulk data sets, which compare across all cell types at once. This was driven by the direct comparison between relatively sparse glial subtypes, which are masked in bulk libraries<sub>10</sub>. This same method has been applied to other disorders including major depression<sub>11</sub>, autism spectrum<sub>12</sub>, and multiple sclerosis<sub>13</sub>.  Secondly, single-cell analysis is used for a higher resolution view at dynamic processes. Time course experiments done in bulk are limited by sampling rate, and generally share the same problem with averaging across many cells, or having to synchronize cell cycle or conditional responses prior to sampling. By using a single-cell approach one can capture and order cells through their progression of a state change<sub>14</sub>. Single-cell analysis allows for a higher resolution view into dynamic processes, and the regulatory landscape across complex tissue (Figure 1).

Single-cell methods have been developed for many epigenomic and genomic assays, however the analyses share a common through-line. Cells are independently, and specifically labelled with a DNA oligonucleotide index that is shared in every sequence read out generated from that cell, such that each read can be assigned back to a specific reaction condition. Single-cell assays work through isolation; tissue or cell cultures are dissociated and single-cells are placed into a reaction vessel. In its simplest form, the reaction vessel is a single tube (Figure 2a). While this strategy tends to perform well on information captured per cell, a “one cell, one well” strategy is limiting in terms of both cost and effort, and these experimental designs tends to suffer from low cell counts. To address this, commercialized products have been developed to increase throughput. Nanowell platforms increase the throughput of the “one cell, one well” strategy by shrinking the well and using specialized means of dispersing the dissociated cell suspension and decreasing reagent cost per cell by limiting volume<sub>15</sub> (Figure 2b).  Alternatively, microfluidic droplet devices use water-oil emulsions as a means to isolate cells into their own partitioned reaction vessels<sub>16–18</sub> (Figure 2c). In these commercialized reactions, each reaction vessel also contains a microbead coated with a single index identifier that is unique, thus labelling the one cell present within the microfluidic droplet uniquely. A limitation that persists is each assay is that they are still bottlenecked by the one cell, one well strategy. This limits throughput and puts a burden of effort and cost on the experimenter. An alternative to these strategies, popularized by us and others, is the use of a split-pool indexing strategy called combinatorial indexing (Figure 2d). Cells can be uniquely labelled without ever being physically isolated. Instead of a single round of uniquely labelling cells, we perform multiple rounds of labelling, with random sampling in between. In this approach, the combination of indexes becomes the unique identifier for each cell. This process is empirically tailored to account for the random chance that multiple cells may follow the same path through library preparation. This is done by limiting the number of cells in the second round of indexing such that the likelihood of any two cell occupying the same well in the first and second round of indexing is sufficiently low<sub>19</sub>. This allows for multi-cell reactions without physical isolation of single cells. This strategy addresses both low cell count, and low assay efficiency concerns at once, driving down experimental cost and effort. 

![Figure 2]({{site.baseurl}}/assets/images/fig2.png){:width="20%"}

Regardless of epigenomic or transcriptomic assay, each single cell captured tends to have low information content, with a non-trivial amount of drop-out<sub>20</sub> due to inefficiency in information capture and inherently low input per reaction. To overcome information drop-out, cells are grouped together based on similarities across the measured moiety. Cells are then aggregated together making multiple “pseudo-bulk” libraries of pure cell types or states — agnostically grouping cells for unbiased analyses. The process of single-cell aggregation also highlights a key concern about experimental design: they must balance information per cell with number of cells sampled. Low information content per single cell requires assumptions to be made about cell grouping, as there is high noise in low-information content system. By having low cell count, there is a risk of losing rare cell types, or having insufficient power for pairwise analyses. This key concern informs the strategy of single-cell capture and the assay used. In the case of cortical development, multiple cellular subtypes are differentiating in parallel, leading to a need for both breadth of cell count and high information per cell to make the most of captured rare events. 
